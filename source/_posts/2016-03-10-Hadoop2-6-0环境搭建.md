---
title: Hadoop2.6.0环境搭建
date: 2016-03-10 17:03:58
tags: hadoop
categories: 技术
---

### 启用root用户 ###
设置root用户密码，以root用户登录，设置方式如下：

    sudo  -s
	gedit  /etc/lightdm/lightdm.conf

修改打开的文件为：

    [SeatDefaults]
	greeter-session=unity-greeter
	user-session=Ubuntu
	greeter-show-manual-login=true
	allow-guest=false

启用root帐号：（Ubuntu默认是禁止root账户的）

    sudo passwd root

设置好密码，重启系统，选择“login”，输入“root”，再输入密码就可以了。

### 免密登陆 ###

配置机器的/etc/hosts和/etc/hostname并安装ssh设置三台机器之间的无密码登录.

#### IP和机器名称的对应关系 ####
在“/etc/hostname”文件中把三台机器的hostname分别设置了SparkMaster、SparkWorker1、SparkWorker2并在每台机器的“/etc/hosts”配置如下IP和机器名称的对应关系：
    
	127.0.0.1	localhost
	192.168.32.131	SparkMaster
	192.168.32.132  SparkWorker1
	192.168.32.133	SparkWorker2

可通过ipconfig来查看ip地址。
可以ping SparkWorker1来查看ip是否配置成功

#### 配置ssh无密码登陆 ####
下面配置ssh无密码登陆：
   
    apt-get install ssh
	/etc/init.d/ssh start   #启动服务
	ps -e |grep ssh  #验证服务是否正常启动
	ssh-keygen -t rsa -P ""  #设置免密登陆，生成私钥和公钥

在/root/.ssh中生成两个文件：id_rsa和id_rsa.pub，id_rsa为私钥，id_rsa.pub为公钥，我们将公钥追加到authorized_keys中，
    
	cat  ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

将SparkWorker1、SparkWorker2的id_rsa.pub传给SparkMaster，使用scp命令进行复制：
SparkWorker1上，
	
	scp ~/.ssh/id_rsa.pub root@SparkMaster:~/.ssh/id_rsa.pub.SparkWorker1

SparkWorker2上，

	scp ~/.ssh/id_rsa.pub root@SparkMaster:~/.ssh/id_rsa.pub.SparkWorker2

然后将公钥添加到SparkMaster的authorized_keys中，
SparkMaster上，

	cd ~/.ssh
	cat id_rsa.pub.SparkWorker1 >> authorized_keys
	cat id_rsa.pub.SparkWorker2 >> authorized_keys

再将SparkMaster的authorized_keys复制到SparkWorker1、SparkWorker2的.ssh目录下：

	scp authorized_keys root@SparkWorker1:~/.ssh/authorized_keys
	scp authorized_keys root@SparkWorker2:~/.ssh/authorized_keys

至此，ssh无密登陆已配置完毕。

	ssh SparkMaster
	ssh SparkWorker1
	ssh SparkWorker2

在一台机器上可以登录其他系统无需密码。

### 配置java环境 ###
SparkMaster上，jdk-8u25-linux-i586.tar.gz

	mkdir /urs/lib/java
	cd /urs/lib/java
	tar -zxvf jdk-8u25-linux-i586.tar.gz
	gedit ~/.bashrc

在最后面添加，后面都用得上

	#JAVA
	export JAVA_HOME=/usr/lib/java/jdk1.8.0_25
	export JRE_HOME=${JAVA_HOME}/jre
	export CLASS_PATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
	export HADOOP_HOME=/usr/local/hadoop/hadoop-2.6.0
	export SCALA_HOME=/usr/lib/scala/scala-2.11.4
	export SPARK_HOME=/usr/local/spark/spark-1.2.0-bin-hadoop2.4
	export IDEA_HOME=/usr/local/idea/idea-IC-139.659.2
	export PATH=${IDEA_HOME}/bin:${SPARK_HOME}/bin:${SCALA_HOME}/bin:${HADOOP_HOME}/bin:${JAVA_HOME}/bin:$PATH

使配置生效:

	source ~/.bashrc

可查看版本号，可验证是否成功。
	
	java -version

在SparkWorker1，SparkWorker2上以同样方法配置，也可通过scp复制。

	scp -r /usr/lib/java/jdk1.8.0_25 root@SparkWorker1:~/usr/lib/java/
	scp -r /usr/lib/java/jdk1.8.0_25 root@SparkWorker2:~/usr/lib/java/
	scp ~/.bashrc root@SparkWorker1:~/.bashrc
	scp ~/.bashrc root@SparkWorker2:~/.bashrc	

复制完成后，在SparkWorker1，SparkWorker2上**`source ~/.bashrc`**使配置生效。

### 配置hadoop环境 ###
SparkMaster上，hadoop-2.6.0.tar.gz

	mkdir /urs/lib/hadoop
	cd /urs/lib/hadoop
	tar -zxvf hadoop-2.6.0.tar.gz
	cd hadoop-2.6.0
	mkdir dfs
	cd dfs
	mkdir name
	mkdir data
	cd ..
	mkdir tmp

接下来开始修改hadoop的配置文件，首先进入Hadoop 2.6.0配置文件区：

	cd etc/hadoop

#### hadoop-env.sh ####
修改配置文件hadoop-env.sh，在其中加入“JAVA_HOME”，指定我们安装的“JAVA_HOME”：

	# The java implementation to use.
	export JAVA_HOME=/usr/lib/java/jdk1.8.0_25

#### yarn-env.sh ####
修改配置文件yarn-env.sh，在其中加入“JAVA_HOME”，

	# some Java parameters
	export JAVA_HOME=/usr/lib/java/jdk1.8.0_25
	if [ "$JAVA_HOME" != "" ]; then
	  #echo "run java in $JAVA_HOME"
	  JAVA_HOME=$JAVA_HOME
	fi

#### mapred-env.sh ####
修改配置文件mapred-env.sh，在其中加入“JAVA_HOME”，如下所示：

	# export JAVA_HOME=/home/y/libexec/jdk1.6.0/
	export JAVA_HOME=/usr/lib/java/jdk1.8.0_25
	
	export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000
	
	export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA

#### slaves ####
修改配置文件slaves，设置Hadoop集群中的从节点为SparkWorker1和SparkWorker2，

	SparkWorker1
	SparkWorker2

#### core-site.xml ####
修改配置文件core-site.xml，如下所示：

	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
		<property>
	        	<name>fs.defaultFS</name>
	                <value>hdfs://SparkMaster:9000</value>
			<description>The name of default file system</description>
	        </property>
	  	<property>
	      		<name>hadoop.tmp.dir</name>
	      	        <value>/home/local/hadoop/hadoop-2.6.0/tmp</value>
			<description>A base for other temporary directories</description>
	        </property>
	</configuration>

#### hdfs-site.xml ####
修改配置文件hdfs-site.xml，如下所示：

	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	
	<configuration>
		<property>
	        	<name>dfs.replication</name>
	        	<value>2</value>
	        </property>
	        <property>
	                <name>dfs.namenode.name.dir</name>
	                <value>/usr/local/hadoop/hadoop-2.6.0/dfs/name</value>
	        </property>
	        <property>
	       		<name>dfs.datanode.data.dir</name>
	                <value>/usr/local/hadoop/hadoop-2.6.0/dfs/data</value>
	        </property>
		<property>
			<name>dfs.http.address</name>
			<value>master:50070</value>
		</property>
		<property>
			<name>dfs.namenode.secondary.http-address</name>
			<value>master:50090</value>
		</property>
	</configuration>

#### mapred-site.xml ####
修改配置文件mapred-site.xml，如下所示：
拷贝一份mapred-site.xml.template命名为mapred-site.xml

	cp mapred-site.xml.template mapred-site.xml

打开mapred-site.xml，

	<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	        <property>	
	        	<name>mapreduce.framework.name</name>
	     		<value>yarn</value>
		</property>
	</configuration>

#### yarn-site.xml ####
修改配置文件yarn-site.xml，如下所示：

	<?xml version="1.0"?>
		<property>
	        	<name>yarn.resourcemanager.hostname</name>
	        	<value>SparkMaster</value>
		</property>
		<property>
	        	<name>yarn.nodemanager.aux-services</name>
	        	<value>mapreduce_shuffle</value>
		</property>
	</configuration>

共有八个配置文件，建议使用scp命令把SparkMaster上安装和配置的Hadoop的各项内容拷贝到SparkWorker1和SparkWorker2上。
![](/images/hadoop_1.png)

### 启动并验证Hadoop分布式集群 ###

#### 第一步：格式化hdfs文件系统 ####
SparkMaster上，

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop namenode -format

#### 第二步：进入sbin中启动hdfs ####
执行如下命令：

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/sbin# ./start-dfs.sh 

此时我们发现在SparkMaster上启动了NameNode和SecondaryNameNode；

![](/images/hadoop_2.png)

在SparkWorker1和SparkWorker2上均启动了DataNode：

![](/images/hadoop_3.png)
![](/images/hadoop_4.png)

每次使用“hadoop namenode -format”命令格式化文件系统的时候会出现一个新的namenodeId，需要把自定义的dfs文件夹的data和name文件夹的内容清空。SparkWorker1和SparkWorker2的也要删掉。

此时访问http://SparkMaster:50070 登录Web控制可以查看HDFS集群的状况：
![](/images/hadoop_5.png)

#### 第三步：启动yarn集群 ####
执行如下命令：
	
	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/sbin# ./start-yarn.sh

使用jps命令可以发现SparkMaster机器上启动了ResourceManager进程：
![](/images/hadoop_6.png)
而在SparkWorker1和SparkWorker2上则分别启动了NodeManager进程：
![](/images/hadoop_7.png)

在SparkMaster上访问http://SparkMaster:8088 可以通过Web控制台查看ResourceManager运行状态：
![](/images/hadoop_8.png)

在SparkMaster上访问http://SparkWorker1:8042  可以通过Web控制台查看SparkWorker1上的NodeManager运行状态：
![](/images/hadoop_9.png)

接下来使用“mr-jobhistory-daemon.sh”来启动JobHistory Server：
![](/images/hadoop_10.png)

启动后可以通过http://SparkMaster:19888 在Web控制台上看到JobHistory中的任务执行历史信息：
![](/images/hadoop_11.png)

结束historyserver的命令如下所示：
![](/images/hadoop_12.png)

#### 第四步：验证Hadoop分布式集群 ####
首先在hdfs文件系统上创建两个目录，创建过程如下所示：

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop fs -mkdir -p /data/wordcount
	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop fs -mkdir -p /output/

Hdfs中的/data/wordcount用来存放Hadoop自带的WordCount例子的数据文件，程序运行的结果输出到/output/wordcount目录中，透过Web控制可以发现我们成功创建了两个文件夹：
![](/images/hadoop_13.png)

接下来将本地文件的数据上传到HDFS文件夹中：

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop fs -put ../etc/hadoop/*.xml  /data/wordcount/

透过Web控制可以发现我们成功上传了文件：
![](/images/hadoop_14.png)

也可通过hadoop的hdfs命令在控制命令终端查看信息：

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop fs -ls /data/wordcount/
![](/images/hadoop_15.png)

运行Hadoop自带的WordCount例子，执行如下命令：

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop jar ../share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /data/wordcount /output/wordcount

当我们在运行作业的过程中也可以查看Web控制台的信息：
![](/images/hadoop_16.png)
![](/images/hadoop_17.png)

程序运行结束后我们可以执行一下命令查看运行结果：

	root@SparkMaster:/usr/local/hadoop/hadoop-2.6.0/bin# hadoop fs -cat /output/wordcount/part-r-00000 |head

![](/images/hadoop_18.png)

可以通过Web控制的JobHistory查看历史工作记录：
![](/images/hadoop_19.png)
![](/images/hadoop_20.png)

发现我们成功运行了WordCount作业。

至此，我们成功构建了Hadoop分布式集群并完成了测试！